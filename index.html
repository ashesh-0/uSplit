<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="HVAE, VAE, Machine learning, Fluorescence microscopy, Image decomposition">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>uSplit: image decomposition for fluorescence microscopy</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">uSplit: image decomposition for fluorescence microscopy</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ashesh-0.github.io/" target="_blank">Ashesh<sup>1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/krull-alexander.aspx" target="_blank">Alexander Krull<sup>2</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/moises-di-sante-8b4396a6" target="_blank"> Moises Di Sante<sup>3</sup></a>,
                  </span>
                  <span class="author-block">
                    <a href="http://www-3.unipv.it/ingserv/servizi/scheda2.php?mat=047899" target="_blank"> Francesco Silvio Pasqualini<sup>3</sup></a>,
                  </span>
                  <span class="author-block">
                    <a href="https://humantechnopole.it/en/people/florian-jug/" target="_blank"> Florian Jug<sup>1</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Human Technopole, Italy; <sup>2</sup>The University of Edinburgh, UK; <sup>3</sup>University of Pavia, Italy<br>ICCV 2023</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2211.12872.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>


                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/juglab/uSplit" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                      </span>


                      <!-- Data link -->
                      <span class="link-block">
                        <a href="https://zenodo.org/record/8235843" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>PaviaATN Dataset</span>
                        </a>
                      </span>

                      <!-- Trained Model link -->
                      <span class="link-block">
                        <a href="https://zenodo.org/record/8235843" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Pre-trained models</span>
                        </a>
                      </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<!-- Paper summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">A brief overview</h2>
        <div class="content has-text-justified">
          <p>
            <b>For Computer Vision enthusiasts,</b> we present a novel meta-architecture for image decomposition. The novelty of the approach lies in its use of large context in a GPU efficient way.<br>
            <b>From microscopists' perspective,</b> the idea is to use a single fluorophore to stain two different structures (like Actin and Tubulin). The obtained image stack will therefore contain superimposed structures. With our machine learning based approach, the goal is to decompose the superimposed image into two channels, each containing one structure.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper summary -->

<!-- Input problem -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <!-- <h2 class="title is-3">Problem Statement</h2> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="item">
          <img src="static/images/problem.png" width="300" height="300" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            <b>The task</b>: Decompose the input patch (left) into two output patches (right). Images taken from notMNIST dataset.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Input problem -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <!-- <h2 class="title is-3">Problem Statement</h2> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="item">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Paper summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <p>
            In this work, the task is to decompose input patch into two constituent output patches. A meta-architecture is proposed which is integrated with Hierarchical VAE and UNet variants.
            The core idea is to use multiple successively lower resolution images alongside the primary input patch. These lower resolution images have the primary input patch in their center and therefore they contain the surrounding regions around the primary input patch. The motivation is to provide additional surrounding context to the network in a GPU memory efficient way. The paper also highlights an issue with the prevalent tiling scheme, provides a hypothesis for it, and proposes a practical way out. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper summary -->


<!-- How does it work -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" align="left">How does it work?</h2>
        <div class="content has-text-justified">
          <p>
            From computer vision perspective, there are two aspects of this work: (a) The<b> meta architecture µSplit</b> and (b) <b>the tiling strategy</b>. 
            <br>
            <b>Our meta architecture, µSplit</b>: Lets start with the intuition behind the meta architecture. To get a good prediction for a pixel, the question we ask is (a) how much surrounding context is needed and (b) at what resolution is it needed. We found that the further away one gets from the pixel in question, it is more often the case that the lower resolution content has sufficient information. Our paper shows this quantitatively. Inline with this empirical observation, the meta architecture makes use of lower and lower resolution images. So, at every hierarchical level, the embedding of the primary input patch, which has undergone downsampling operation, gets merged with the embedding of an  equally downsampled (~lower resolution) neighboring patch. 
            
            <br>Lets take a concrete example. Given a 64x64 sized patch, the embeddings of the primary patch at successive hierarchical levels will have embeddings with spatial dimensions of 32X32, 16x16, 8x8 sizes and so on. At every hierarchical level, we feed 64x64 sized low resolution patch which are created by downsampling the whole input frame two times, four times, eight times and so on and selecting a 64x64 sized patch, which is centered on the primary input patch. Two things are to be noted: At every hierarchical level, resolution level of the primary patch embedding matches the resolution level of downsampled low resolution image which is fed at that hierarchy level. Secondly, these more and more downsampled patches contain lower and lower resolution content of larger and larger regions surrounding the input patch. In this way, a large context is fed to the network in a memory efficient way. 
            <br>
            <b>The tiling strategy:</b> For predicting on large input frames, one first predicts on smaller patches and then stitches them together. The tiling strategy is to ensure that the stitching is smooth, i.e, there are no boundary artefacts. In this work, we found that if one uses a much bigger patch size at evaluation time than what was used during training, the performance degrades. This is what happens in the prevalent tiling strategy, which the paper calls 'Outer Padding'. Paper provides a hypothesis as to why this happens and proposes using the same patch size both during evaluation and training, which is called 'Inner Padding'.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- How does it work -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        
        <!-- Results -->
        <div class="item">
          <h2 class="title is-3">Qualitative Results</h2>
          <!-- Your image here -->
          <img src="static/images/ActvsMito.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Qualitative results on the Act vs. Mito task from Hagen et al dataset. For the input patch (1st col, 2nd row), we compare ground truth to results obtained with the vanilla HVAE baseline trained with a patch size of 64 to results obtained with two variations of µSplit (HVAEs using Lean-LC and Deep-LC, both also using a patch size of 64). The overlaid histograms shows either the intensity distribution of the two channels (column 1) or the intensity distribution of the ground truth and the prediction (red). The given PSNR are for the individual prediction (full input image) and for the entire dataset (in brackets).
          </h2>
        </div>

       <div class="item">
        <!-- Your image here -->
        <img src="static/images/LC_piu_DeepLC.png" alt="Overall architecture"/>
        <h2 class="subtitle has-text-centered">
          Network architecture of µSplit. Left: We show the network architecture employed by Regular-LC. The input (left side) consists of a core image patch (bottom left), together with downscaled version of the patch surroundings – the lateral context (LC). We show the area corresponding to the original patch as red dotted box throughout the figure. Right: The network architecture of Deep-LC. A Vanilla HVAE architecture is stacked on top of the Regular-LC architecture shown in Left figure. Note that this is only possible because the latent space in Regular-LC retained the spatial dimensions of all layers by means of using the proposed LC. 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/LC_lean.drawio.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Our Lean-LC and modules used in it (TopDown block, Residual block) which were taken from <a href="https://arxiv.org/abs/2006.06072">Prakash et al.</a> Here, the latent embeddings passed to the top-down block are first centercropped to appropriate size so that the top-down block of a Vanilla HVAE can be used. 
       </h2>
     </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2> -->
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <!-- </div> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <h2 class="title is-3">Another Carousel</h2> -->
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <!-- <div class="item item-video1"> -->
          <!-- <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video2"> -->
          <!-- <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video3"> -->
          <!-- <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End video carousel -->
<!-- Paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @InProceedings{ashesh_2023_ICCV,
          author       = {Ashesh and
                          Krull, Alexander and
                          Di Sante, Moises and
                          Silvio Pasqualini, Francesco and
                          Jug, Florian},
          title        = {uSplit: image decomposition for fluorescence microscopy},
          month        = {October},
          year         = {2023},
          booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
